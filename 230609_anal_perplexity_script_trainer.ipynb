{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델은 반드시 gpt-2 데이터 다르게"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCRIPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Library and Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rds/general/user/jj1122/home/anaconda3/envs/m2d2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast, GPT2Tokenizer, set_seed\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import torch\n",
    "import argparse\n",
    "from datetime import date, datetime\n",
    "from os import listdir\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path_format = \"/rds/general/user/jj1122/home/projects/m2d2/dataset/{model_id}/models\"\n",
    "# ckpt_path_format = \"/checkpoint-{ckpt}\"\n",
    "cache_dir = \"/rds/general/user/jj1122/ephemeral/.cache/huggingface\"\n",
    "# General_referece\n",
    "# list_of_datasets = [\n",
    "#     \"cs_l1\",\n",
    "#     \"nlin_l1\",\n",
    "#     \"Health_and_fitness\",\n",
    "#     \"Natural_and_physical_sciences\",\n",
    "#     \"Religion_and_belief_systems\",\n",
    "#     \"Culture_and_the_arts\",\n",
    "#     \"General_referece\",\n",
    "#     \"econ_l1\",\n",
    "#     \"History_and_events\",\n",
    "#     \"Human_activites\",\n",
    "#     \"Mathematics_and_logic\",\n",
    "# #     \"astro-ph_l1\",\n",
    "# #     \"cond-mat_l1\",\n",
    "#     \"eess_l1\",\n",
    "# #     \"math_l1\",\n",
    "# #     \"physics_l1 (ERROR)\",\n",
    "#     \"q-bio_l1\",\n",
    "#     \"q-fin_l1\",\n",
    "#     \"stat_l1\",\n",
    "#     \"Philosophy\",\n",
    "#     \"Philosophy_and_thinking\",\n",
    "#     \"Society_and_social_sciences\",\n",
    "#     \"Technology_and_applied_sciences\",\n",
    "\n",
    "# ]\n",
    "\n",
    "device_id = \"cuda\"\n",
    "today_dt = date.today().strftime(\"%y%m%d\")\n",
    "\n",
    "model_id = \"gpt2\"\n",
    "# model_type = \"zsh\"\n",
    "\n",
    "# output_file = f\"./output_logs/{today_dt}_{model_type}.json\"\n",
    "# ds = \"Religion_and_belief_systems\"\n",
    "# ds = \"econ_l1\"\n",
    "ds = \"physics_l1\"\n",
    "# General_referece\n",
    "model_path = f\"/rds/general/user/jj1122/home/projects/m2d2/dataset/{ds}/models\"\n",
    "\n",
    "# econ_l1\n",
    "stride = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model, Tokeniser, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset m2d2 (/rds/general/user/jj1122/ephemeral/.cache/huggingface/machelreid___m2d2/physics_l1/0.0.0/eb235f33a5de3163c10549b7f63c906910539c8a8c0ec5ade1285ccbf5067d00)\n",
      "100%|██████████| 3/3 [00:05<00:00,  1.97s/it]\n"
     ]
    }
   ],
   "source": [
    "set_seed(718)\n",
    "# model = GPT2LMHeadModel.from_pretrained(model_path, cache_dir=cache_dir).to(device_id)\n",
    "# model.eval()\n",
    "# tokenizer = GPT2TokenizerFast.from_pretrained(model_path)\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"machelreid/m2d2\", ds, cache_dir=cache_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenisation & Grouping Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2395/2395 [00:00<00:00, 8285.61it/s]\n"
     ]
    }
   ],
   "source": [
    "dict_input_ids = {}\n",
    "\n",
    "for gubun in [\"validation\"]: # [\"train\", \"validation\", \"test\"]:\n",
    "    if gubun == \"train\": continue\n",
    "\n",
    "    # TOKENISATION\n",
    "    encodings = tokenizer(\"\\n\".join(dataset[gubun][\"text\"]), return_tensors=\"pt\")\n",
    "\n",
    "    # GROUPING TEXT\n",
    "    with torch.no_grad():\n",
    "        max_length = model.config.n_positions\n",
    "        nlls = []\n",
    "        list_input_ids = []\n",
    "        for i in tqdm(range(0, encodings.input_ids.size(1), stride)):\n",
    "            begin_loc = max(i + stride - max_length, 0)\n",
    "            end_loc = min(i + stride, encodings.input_ids.size(1))\n",
    "            trg_len = end_loc - i  # may be different from stride on last loop\n",
    "            input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device_id)\n",
    "            target_ids = input_ids.clone()\n",
    "            target_ids[:, :-trg_len] = -100\n",
    "            list_input_ids.append(input_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset[gubun][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "## Raw Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In full agreement with the results of the new quantum theory directional quantization appears as a general and universal feature of quantum measurements. One experimental example for such directional quantization in scattering processes is shown. Last not least, the early history of the \"almost\" discovery of the electron spin in the SGE is revisited.',\n",
       " \"In almost all introductory textbooks on atomic physics the Stern-Gerlach experiment (SGE), performed by Otto Stern (1888 Stern ( -1969 and Walther Gerlach (1889 -1979 in Frankfurt in 1922 (Stern, 1920a; Gerlach and Stern, 1921 , 1922b Gerlach, 1925) , is presented as a benchmark experiment of quantum science. In most textbooks, the SGE is taken as evidence for proving that Pieter Debye's (Debye, 1916) and Arnold Sommerfeld's (Sommerfeld, 1916) hypothesis of directional quantization of magnetic and electric momenta of quantum objects in the presence of electric and magnetic fields is a real fact in the quantum world and that magnetic momenta in atoms are quantized. But a more fundamental milestone result, we emphasize, is to be seen in the fact that the SGE provided the first experimental evidence that, in fact, all angular momenta are quantized in all quantum systems.\",\n",
       " \"Soon after the advent of the new quantum mechanics, the SGE was recognized as a key experiment to study and understand the problem of measurement in the new theory. As such it was discussed already at the 1927 Solvay conference (Bacciagaluppi and Valentini, 2009, esp. pp. 436, 478) and by Werner Heisenberg in his paper on the uncertainty relation (Heisenberg, 1927) . It was then discussed as the paradigmatic example of a theory of the quantum measurement process in David Bohm's textbook on quantum theory (Bohm, 1951, ch. 22) . In recent years, the SGE has been discussed by many authors.\",\n",
       " 'More sophisticated descriptions of the SGE from a physical point of view were presented by Scully et al. (1978) ; Mackintosh (1983) ; Scully et al. (1987) ; Reinisch (1999) . Because of its significance as a paradigm for the quantum measurement problem, the SGE has also been discussed both from a historical and a philosophical point of view, see (Bernstein, 2010) , (Sauer, 2016) . Weinert (1995) has made the point that the experiment was designed based on a wrong theory, but proved to be the right experiment.',\n",
       " 'A reconstruction of the historical experiment has been the study of a recent doctoral dissertation (Trageser, 2011) . As a part of this dissertation, the experiment was partially rebuilt, replications of other aspects were reported by Herschbach (2003, 2005) . The historical theoretical context and, in particular, the role of the SGE for the development of the new quantum mechanics was the topic of another recent dissertation (Pié i Valls, 2015) . It was also discussed from a didactic perspective, e.g. by French and Taylor (1978) , or by Platt (1990) .']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"validation\"][\"text\"][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([23799,    41,    57,    43,   220, 13695,   270,  1757,   366,    51,\n",
      "        13411,     1,  4373,   357,  6286,   807,  3035,  9656,   828,  1365])\n"
     ]
    }
   ],
   "source": [
    "print(encodings['input_ids'][0][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ĠTB', 'J', 'Z', 'L', 'Ġ', 'ĠTob', 'it', 'ĠJohn', 'Ġ\"', 'T', 'obi', '\"', 'ĠBrown', 'Ġ(', 'born', 'Ġ8', 'ĠApril', 'Ġ1993', '),', 'Ġbetter']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(encodings['input_ids'][0][:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2451922"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encodings['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2451922"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([x for x in list(chain(*(encodings['input_ids']))) if x not in [198, 628] ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([x for x in list(chain(*(encodings['input_ids']))) if x in [198, 628] ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  198, 22737,    41,  ...,   587,  1444,   262]], device='cuda:0')"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_input_ids[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ċ', 'TB', 'J', 'Z', 'L', 'Ċ', 'Ċ', 'T', 'ob', 'it']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(list_input_ids[0][0][:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2443"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script to Trainer pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 1394404\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 55451\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 55451\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /rds/general/user/jj1122/ephemeral/.cache/huggingface/machelreid___m2d2/General_referece/0.0.0/eb235f33a5de3163c10549b7f63c906910539c8a8c0ec5ade1285ccbf5067d00/cache-c48f7bc4f6e010f1_*_of_00008.arrow\n",
      "Loading cached processed dataset at /rds/general/user/jj1122/ephemeral/.cache/huggingface/machelreid___m2d2/General_referece/0.0.0/eb235f33a5de3163c10549b7f63c906910539c8a8c0ec5ade1285ccbf5067d00/cache-e6a697a7f547c421_*_of_00008.arrow\n",
      "Loading cached processed dataset at /rds/general/user/jj1122/ephemeral/.cache/huggingface/machelreid___m2d2/General_referece/0.0.0/eb235f33a5de3163c10549b7f63c906910539c8a8c0ec5ade1285ccbf5067d00/cache-4023b72209bfaf59_*_of_00008.arrow\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    output = tokenizer(examples['text'])\n",
    "    # clm input could be much much longer than block_size\n",
    "    return output\n",
    "    \n",
    "tokenized_datasets = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    num_proc=8,\n",
    "    remove_columns=['text'],\n",
    "    load_from_cache_file=True,\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2449110"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(chain(*(tokenized_datasets['validation']['input_ids']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([x for x in list(chain(*(tokenized_datasets['validation']['input_ids']))) if x == 198 ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## group_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /rds/general/user/jj1122/ephemeral/.cache/huggingface/machelreid___m2d2/General_referece/0.0.0/eb235f33a5de3163c10549b7f63c906910539c8a8c0ec5ade1285ccbf5067d00/cache-f04adc3e638b2ad4_*_of_00008.arrow\n",
      "Loading cached processed dataset at /rds/general/user/jj1122/ephemeral/.cache/huggingface/machelreid___m2d2/General_referece/0.0.0/eb235f33a5de3163c10549b7f63c906910539c8a8c0ec5ade1285ccbf5067d00/cache-1fbdd863da8a8924_*_of_00008.arrow\n",
      "Loading cached processed dataset at /rds/general/user/jj1122/ephemeral/.cache/huggingface/machelreid___m2d2/General_referece/0.0.0/eb235f33a5de3163c10549b7f63c906910539c8a8c0ec5ade1285ccbf5067d00/cache-daadfba2f078454b_*_of_00008.arrow\n"
     ]
    }
   ],
   "source": [
    "block_size = tokenizer.model_max_length\n",
    "if block_size > 1024:\n",
    "    block_size = 1024\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    if total_length >= block_size:\n",
    "        total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    num_proc=8,\n",
    "    load_from_cache_file=True,\n",
    "    desc=f\"Grouping texts in chunks of {block_size}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'TBJZL',\n",
       " '',\n",
       " 'Tobit John \"Tobi\" Brown (born 8 April 1993), better known as TBJZL, is an English YouTuber, streamer, Internet personality, and rapper. He is also a member and co-founder of the British YouTube group known as the Sidemen.',\n",
       " 'In 2019, Brown was listed as the 38th most influential online creator in the United Kingdom by \"The Sunday Times\". , his main YouTube channel has over 4 million subscribers and 400 million video views. Brown has also released music mononymously as Tobi, and in 2020, he released his debut single \"Destined for Greatness\", which reached number 31 on the UK Singles Chart.']"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"validation\"][\"text\"][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 1394404\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 55451\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 55451\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [22737, 41, 57, 43], [], [51, 672, 270, 1757, 366, 51, 13411, 1, 4373, 357, 6286, 807, 3035, 9656, 828, 1365, 1900, 355, 23799, 41, 57, 43, 11, 318, 281, 3594, 921, 51, 18478, 11, 4269, 263, 11, 4455, 8806, 11, 290, 25670, 13, 679, 318, 635, 257, 2888, 290, 763, 12, 15454, 286, 262, 3517, 7444, 1448, 1900, 355, 262, 15686, 8952, 13], [818, 13130, 11, 4373, 373, 5610, 355, 262, 4353, 400, 749, 14212, 2691, 13172, 287, 262, 1578, 7526, 416, 366, 464, 3502, 3782, 1911, 837, 465, 1388, 7444, 6518, 468, 625, 604, 1510, 18327, 290, 7337, 1510, 2008, 5009, 13, 4373, 468, 635, 2716, 2647, 937, 5177, 3481, 355, 309, 13411, 11, 290, 287, 12131, 11, 339, 2716, 465, 8886, 2060, 366, 24159, 1389, 329, 3878, 1108, 1600, 543, 4251, 1271, 3261, 319, 262, 3482, 5573, 829, 22086, 13]]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_datasets[\"validation\"][\"input_ids\"][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TB', 'J', 'Z', 'L', 'T', 'ob', 'it', 'ĠJohn', 'Ġ\"', 'T', 'obi', '\"', 'ĠBrown', 'Ġ(', 'born', 'Ġ8', 'ĠApril', 'Ġ1993', '),', 'Ġbetter', 'Ġknown', 'Ġas', 'ĠTB', 'J', 'Z', 'L', ',', 'Ġis', 'Ġan', 'ĠEnglish', 'ĠYou', 'T', 'uber', ',', 'Ġstream', 'er', ',', 'ĠInternet', 'Ġpersonality', ',', 'Ġand', 'Ġrapper', '.', 'ĠHe', 'Ġis', 'Ġalso', 'Ġa', 'Ġmember', 'Ġand', 'Ġco', '-', 'founder', 'Ġof', 'Ġthe', 'ĠBritish', 'ĠYouTube', 'Ġgroup', 'Ġknown', 'Ġas', 'Ġthe', 'ĠSid', 'emen', '.', 'In', 'Ġ2019', ',', 'ĠBrown', 'Ġwas', 'Ġlisted', 'Ġas', 'Ġthe', 'Ġ38', 'th', 'Ġmost', 'Ġinfluential', 'Ġonline', 'Ġcreator', 'Ġin', 'Ġthe', 'ĠUnited', 'ĠKingdom', 'Ġby', 'Ġ\"', 'The', 'ĠSunday', 'ĠTimes', '\".', 'Ġ,', 'Ġhis', 'Ġmain', 'ĠYouTube', 'Ġchannel', 'Ġhas', 'Ġover', 'Ġ4', 'Ġmillion', 'Ġsubscribers', 'Ġand', 'Ġ400', 'Ġmillion', 'Ġvideo', 'Ġviews', '.', 'ĠBrown', 'Ġhas', 'Ġalso', 'Ġreleased', 'Ġmusic', 'Ġmon', 'onym', 'ously', 'Ġas', 'ĠT', 'obi', ',', 'Ġand', 'Ġin', 'Ġ2020', ',', 'Ġhe', 'Ġreleased', 'Ġhis', 'Ġdebut', 'Ġsingle', 'Ġ\"', 'Dest', 'ined', 'Ġfor', 'ĠGreat', 'ness', '\",', 'Ġwhich', 'Ġreached', 'Ġnumber', 'Ġ31', 'Ġon', 'Ġthe', 'ĠUK', 'ĠSing', 'les', 'ĠChart', '.']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(chain(*tokenized_datasets[\"validation\"][\"input_ids\"][:5])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_token_diff = set(encodings['input_ids'][0].tolist()) \\\n",
    "    - set(list(chain(*tokenized_datasets[\"validation\"][\"input_ids\"])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{198, 628}"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_token_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_token_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ċ', 'ĊĊ']"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens([198, 628])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2449110"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(chain(*tokenized_datasets[\"validation\"][\"input_ids\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 76543\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 2364\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 2290\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[22737, 41, 57, 43, 51]"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_datasets['validation'][0]['input_ids'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TB', 'J', 'Z', 'L', 'T']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(lm_datasets['validation'][0]['input_ids'][:5]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2364"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lm_datasets['validation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perplexity with Trainer pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nlls = []\n",
    "def eval_ppl_v3(model, tokenized_datasets, stride, device=device_id):\n",
    "    max_length = model.config.n_positions\n",
    "    nlls = []\n",
    "    encodings = list(chain(*(tokenized_datasets['validation']['input_ids'])))\n",
    "    n_words = len(encodings)\n",
    "\n",
    "    encodings = torch.tensor([encodings])\n",
    "    \n",
    "    print(len(list(range(0, n_words, stride))))\n",
    "    for i in tqdm(range(0, n_words, stride)):\n",
    "        begin_loc = max(i + stride - max_length, 0)\n",
    "        end_loc = min(i + stride, n_words)\n",
    "        trg_len = end_loc - i  # may be different from stride on last loop\n",
    "        # input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n",
    "        input_ids = encodings[:, begin_loc:end_loc].to(device)\n",
    "        target_ids = input_ids.clone()\n",
    "        target_ids[:, :-trg_len] = -100\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, labels=target_ids)\n",
    "            neg_log_likelihood = outputs[0] * trg_len\n",
    "\n",
    "        nlls.append(neg_log_likelihood)\n",
    "\n",
    "    ppl = torch.exp(torch.stack(nlls).sum() / end_loc)\n",
    "    return ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2392/2392 [01:28<00:00, 27.16it/s]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    ppl = eval_ppl_v3(model, tokenized_datasets, stride=1024).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nlls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.250993728637695"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encodings format 한번더 확 인\n",
    "encodings = list(chain(*tokenized_datasets[\"validation\"][\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/rds/general/user/jj1122/home/projects/domain_adaptation_m2d2\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "m2d2_env",
   "language": "python",
   "name": "m2d2_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
