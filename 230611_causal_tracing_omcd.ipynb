{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 230611_causal_tracing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch 처리 가능하게  - loss 땜시 불가능  \n",
    "tokenisation 방식 변경 여러개"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rds/general/user/jj1122/home/anaconda3/envs/m2d2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast, GPT2Tokenizer, set_seed\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import torch\n",
    "import argparse\n",
    "import datasets\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "from datetime import date\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(718)\n",
    "set_seed(718)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_cd = \"omcd_Religion_and_belief_systems\"\n",
    "# job_cd = \"tmod_Religion_and_belief_systems\"\n",
    "# job_cd = sys.argv[2] # TODO\n",
    "\n",
    "list_job_cd = job_cd.split(\"_\")\n",
    "job_gubun = list_job_cd[0]\n",
    "model_id = \"_\".join(list_job_cd[1:])\n",
    "\n",
    "cache_dir = \"/rds/general/user/jj1122/ephemeral/.cache/huggingface\"\n",
    "device_id = \"cpu\"\n",
    "\n",
    "n_layers = 12\n",
    "list_modules = ['attn', 'mlp']\n",
    "trace_module_id = \"transformer.h.{l}.{m}\"\n",
    "\n",
    "tuned_model_path = f\"/rds/general/user/jj1122/home/projects/m2d2/dataset/{model_id}/models\"\n",
    "\n",
    "today_dt = date.today().strftime(\"%y%m%d\")\n",
    "output_file = f\"/rds/general/user/jj1122/home/projects/m2d2/utils/output_logs/{today_dt}_{job_gubun}_{model_id}.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_trace_module_ids = []\n",
    "\n",
    "for l in range(n_layers):\n",
    "    for m in list_modules:\n",
    "        list_trace_module_ids.append(trace_module_id.format(l=l, m=m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset m2d2 (/rds/general/user/jj1122/ephemeral/.cache/huggingface/machelreid___m2d2/Religion_and_belief_systems/0.0.0/eb235f33a5de3163c10549b7f63c906910539c8a8c0ec5ade1285ccbf5067d00)\n",
      "100%|██████████| 3/3 [00:00<00:00, 276.90it/s]\n",
      "Loading cached processed dataset at /rds/general/user/jj1122/ephemeral/.cache/huggingface/machelreid___m2d2/Religion_and_belief_systems/0.0.0/eb235f33a5de3163c10549b7f63c906910539c8a8c0ec5ade1285ccbf5067d00/cache-b257c9c3236091fa.arrow\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"machelreid/m2d2\", model_id, cache_dir=cache_dir)\n",
    "ds_test = dataset[\"test\"].filter(lambda x: x['text'] != '')\n",
    "# gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /rds/general/user/jj1122/ephemeral/.cache/huggingface/machelreid___m2d2/Religion_and_belief_systems/0.0.0/eb235f33a5de3163c10549b7f63c906910539c8a8c0ec5ade1285ccbf5067d00/cache-3708937b1d6e8848_*_of_00008.arrow\n"
     ]
    }
   ],
   "source": [
    "set_seed(718)\n",
    "\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    output = gpt2_tokenizer(examples['text'])\n",
    "    return output\n",
    "\n",
    "tokenized_datasets = ds_test.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    num_proc=8,\n",
    "    remove_columns='text', #TODO\n",
    "    load_from_cache_file=True,\n",
    ")\n",
    "\n",
    "len_sentences = len(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    return GPT2LMHeadModel.from_pretrained(tuned_model_path).to(device_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OMCD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x14ead6342590>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First run: clean run def\n",
    "def save_clean_activation(m_id):\n",
    "    def save_clean_activation_hook(module, _input, _output):\n",
    "#         print(m_id, _output.shape)\n",
    "        if m_id.endswith('attn'):\n",
    "            clean_activations[m_id] = _output[0].detach()\n",
    "        elif m_id.endswith('mlp'):\n",
    "#         else:\n",
    "            clean_activations[m_id] = _output.detach()\n",
    "    return save_clean_activation_hook\n",
    "\n",
    "tuned_model = load_model()\n",
    "tuned_model.eval()\n",
    "for m_id in list_trace_module_ids:\n",
    "    tuned_model.get_submodule(m_id).register_forward_hook(save_clean_activation(m_id))    \n",
    "\n",
    "    \n",
    "# Second run: corruped run def    \n",
    "def corrupt_input_vector(module, _input):#, _output):\n",
    "    torch.manual_seed(718)\n",
    "    std = torch.std(_input[0])\n",
    "    return tuple([_input[0] + (std*1.5) * torch.randn(_input[0].shape).to(device_id), ])\n",
    "\n",
    "corrupted_model = load_model()\n",
    "corrupted_model.eval()\n",
    "corrupted_model.get_submodule(\"transformer.h.0.attn\").register_forward_pre_hook(corrupt_input_vector)    \n",
    "\n",
    "\n",
    "# Third run: restored run def    \n",
    "def restore_activation(m_id):\n",
    "    def restore_activation_hook(module, _input, _output):\n",
    "        clean_activation = clean_activations[m_id]#[:, t]\n",
    "        if m_id.endswith('attn'):\n",
    "            return tuple([clean_activation, tuple([_output[1][0], _output[1][1]])])\n",
    "        elif m_id.endswith('mlp'):           \n",
    "            base_output = _output.detach()\n",
    "            base_output = clean_activation\n",
    "            return base_output\n",
    "    return restore_activation_hook\n",
    "\n",
    "restored_model = load_model()\n",
    "restored_model.eval()\n",
    "restored_model.get_submodule(\"transformer.h.0.attn\").register_forward_pre_hook(corrupt_input_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x14876b9255a0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First run: clean run def\n",
    "def save_clean_activation_v2(m_id):\n",
    "    def save_clean_activation_hook_v2(module, _input, _output):\n",
    "#         print(m_id, _output.shape)\n",
    "        if m_id.endswith('attn'):\n",
    "            clean_activations[m_id] = _output\n",
    "        elif m_id.endswith('mlp'):\n",
    "#         else:\n",
    "            clean_activations[m_id] = _output\n",
    "    return save_clean_activation_hook_v2\n",
    "\n",
    "tuned_model = load_model()\n",
    "tuned_model.eval()\n",
    "for m_id in list_trace_module_ids:\n",
    "    tuned_model.get_submodule(m_id).register_forward_hook(save_clean_activation_v2(m_id))\n",
    "\n",
    "    \n",
    "# Second run: corruped run def    \n",
    "def corrupt_input_vector_v2(module, _input):#, _output):\n",
    "    torch.manual_seed(718)\n",
    "    std = torch.std(_input[0])\n",
    "    return tuple([_input[0] + (std*1.5) * torch.randn(_input[0].shape).to(device_id), ])\n",
    "\n",
    "corrupted_model = load_model()\n",
    "corrupted_model.eval()\n",
    "corrupted_model.get_submodule(\"transformer.h.0.attn\").register_forward_pre_hook(corrupt_input_vector_v2)\n",
    "\n",
    "\n",
    "# Third run: restored run def    \n",
    "def restore_activation_v2(m_id):\n",
    "    def restore_activation_hook_v2(module, _input, _output):\n",
    "        clean_activation = clean_activations[m_id]#[:, t]\n",
    "        if m_id.endswith('attn'):\n",
    "            return clean_activation\n",
    "        elif m_id.endswith('mlp'):           \n",
    "#             base_output = _output.detach()\n",
    "#             base_output = clean_activation\n",
    "            return clean_activation\n",
    "    return restore_activation_hook_v2\n",
    "\n",
    "restored_model = load_model()\n",
    "restored_model.eval()\n",
    "restored_model.get_submodule(\"transformer.h.0.attn\").register_forward_pre_hook(corrupt_input_vector_v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causal Tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with open(output_file, 'r') as json_file:\n",
    "        list_results = json.load(json_file)\n",
    "    for i, d in enumerate(list_results):\n",
    "        if len(d) == 0:\n",
    "            done_idx = i - 1\n",
    "            break\n",
    "except:\n",
    "    list_results = [{} for x in range(len_sentences)]\n",
    "    done_idx = 0\n",
    "print(done_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "done_idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask'],\n",
       "    num_rows: 3\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets.select([0, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence_idx: 0\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(718)\n",
    "set_seed(718)\n",
    "\n",
    "for sentence_idx, data in enumerate(tokenized_datasets.select([0, 1, 2])):\n",
    "    if done_idx > sentence_idx: continue\n",
    "    if sentence_idx % 1000 == 0:\n",
    "        print(f\"sentence_idx: {sentence_idx}\")\n",
    "        with open(output_file, 'w') as json_file:\n",
    "            json.dump(list_results, json_file)\n",
    "    \n",
    "    inputs = torch.tensor(data['input_ids']).to(device_id)\n",
    "    \n",
    "    # First run: clean run        \n",
    "    clean_activations = {}\n",
    "    with torch.no_grad():\n",
    "        clean_outputs = tuned_model(inputs, labels=inputs.clone())\n",
    "        clean_loss = np.exp(clean_outputs.loss.item())\n",
    "\n",
    "    # Second run: corrupted run    \n",
    "    with torch.no_grad():\n",
    "        corrupted_outputs = corrupted_model(inputs, labels=inputs.clone())\n",
    "        corrupted_loss = np.exp(corrupted_outputs.loss.item())\n",
    "        \n",
    "    # Third run: corrupted-with-restoration run    \n",
    "    restored_loss = {}\n",
    "    with torch.no_grad():\n",
    "        for m_id in list_trace_module_ids:\n",
    "            hook = restored_model.get_submodule(m_id).register_forward_hook(restore_activation(m_id))\n",
    "            restored_outputs = restored_model(inputs, labels=inputs.clone())\n",
    "            restored_loss[m_id] = np.exp(restored_outputs.loss.item())\n",
    "            hook.remove()\n",
    "            \n",
    "    list_results[sentence_idx]['clean_loss'] = clean_loss\n",
    "    list_results[sentence_idx]['corrupted_loss'] = corrupted_loss\n",
    "    list_results[sentence_idx]['restored_loss'] = restored_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'clean_loss': 63.91732102791386,\n",
       "  'corrupted_loss': 132.8242889027916,\n",
       "  'restored_loss': {'transformer.h.0.attn': 63.91732102791386,\n",
       "   'transformer.h.0.mlp': 72.45838449838195,\n",
       "   'transformer.h.1.attn': 108.48160917896826,\n",
       "   'transformer.h.1.mlp': 98.25952101895331,\n",
       "   'transformer.h.2.attn': 105.73393221716806,\n",
       "   'transformer.h.2.mlp': 157.6101965283722,\n",
       "   'transformer.h.3.attn': 114.146045922409,\n",
       "   'transformer.h.3.mlp': 115.46617860310457,\n",
       "   'transformer.h.4.attn': 100.84806083556465,\n",
       "   'transformer.h.4.mlp': 95.89193990231547,\n",
       "   'transformer.h.5.attn': 131.30281967082848,\n",
       "   'transformer.h.5.mlp': 97.5344761810575,\n",
       "   'transformer.h.6.attn': 102.20784520792095,\n",
       "   'transformer.h.6.mlp': 109.59660862325092,\n",
       "   'transformer.h.7.attn': 106.16173384104698,\n",
       "   'transformer.h.7.mlp': 95.2186388402459,\n",
       "   'transformer.h.8.attn': 112.70963296738027,\n",
       "   'transformer.h.8.mlp': 108.70634142756694,\n",
       "   'transformer.h.9.attn': 127.1640592432581,\n",
       "   'transformer.h.9.mlp': 84.25118249435614,\n",
       "   'transformer.h.10.attn': 125.37441122644063,\n",
       "   'transformer.h.10.mlp': 118.2573743814645,\n",
       "   'transformer.h.11.attn': 115.2849580506915,\n",
       "   'transformer.h.11.mlp': 117.34240362818905}},\n",
       " {'clean_loss': 33.48678344461458,\n",
       "  'corrupted_loss': 60.98568078003749,\n",
       "  'restored_loss': {'transformer.h.0.attn': 33.48678344461458,\n",
       "   'transformer.h.0.mlp': 37.003198415471566,\n",
       "   'transformer.h.1.attn': 64.097670397889,\n",
       "   'transformer.h.1.mlp': 72.50092908024622,\n",
       "   'transformer.h.2.attn': 48.12944801536147,\n",
       "   'transformer.h.2.mlp': 55.87564014050752,\n",
       "   'transformer.h.3.attn': 49.198414002287066,\n",
       "   'transformer.h.3.mlp': 56.622639772611144,\n",
       "   'transformer.h.4.attn': 52.96746860053392,\n",
       "   'transformer.h.4.mlp': 58.081943900922326,\n",
       "   'transformer.h.5.attn': 47.15663012977383,\n",
       "   'transformer.h.5.mlp': 58.93682358602892,\n",
       "   'transformer.h.6.attn': 50.619899921520926,\n",
       "   'transformer.h.6.mlp': 52.43684580887924,\n",
       "   'transformer.h.7.attn': 49.19644343267088,\n",
       "   'transformer.h.7.mlp': 48.81173161719443,\n",
       "   'transformer.h.8.attn': 49.08029601688048,\n",
       "   'transformer.h.8.mlp': 52.709431077000666,\n",
       "   'transformer.h.9.attn': 53.61403110579979,\n",
       "   'transformer.h.9.mlp': 48.12371088002379,\n",
       "   'transformer.h.10.attn': 55.51620696007936,\n",
       "   'transformer.h.10.mlp': 49.97331926353094,\n",
       "   'transformer.h.11.attn': 57.22955743146801,\n",
       "   'transformer.h.11.mlp': 54.953368785640976}},\n",
       " {'clean_loss': 14.614322888160343,\n",
       "  'corrupted_loss': 178.84399700976167,\n",
       "  'restored_loss': {'transformer.h.0.attn': 14.614322888160343,\n",
       "   'transformer.h.0.mlp': 24.60711412079997,\n",
       "   'transformer.h.1.attn': 222.1300657441556,\n",
       "   'transformer.h.1.mlp': 107.46851631638697,\n",
       "   'transformer.h.2.attn': 137.68372165579638,\n",
       "   'transformer.h.2.mlp': 202.31722056913154,\n",
       "   'transformer.h.3.attn': 74.8837955315738,\n",
       "   'transformer.h.3.mlp': 116.30228236906993,\n",
       "   'transformer.h.4.attn': 102.4402409559173,\n",
       "   'transformer.h.4.mlp': 112.37750080436398,\n",
       "   'transformer.h.5.attn': 95.64849033743964,\n",
       "   'transformer.h.5.mlp': 84.30201805425571,\n",
       "   'transformer.h.6.attn': 124.62934418060337,\n",
       "   'transformer.h.6.mlp': 75.63842723730501,\n",
       "   'transformer.h.7.attn': 162.58746243439813,\n",
       "   'transformer.h.7.mlp': 79.85231997278244,\n",
       "   'transformer.h.8.attn': 140.8238425421443,\n",
       "   'transformer.h.8.mlp': 107.7130789954823,\n",
       "   'transformer.h.9.attn': 165.1277627333633,\n",
       "   'transformer.h.9.mlp': 128.27489363427617,\n",
       "   'transformer.h.10.attn': 170.65087917703588,\n",
       "   'transformer.h.10.mlp': 66.52228107764671,\n",
       "   'transformer.h.11.attn': 147.28270691984204,\n",
       "   'transformer.h.11.mlp': 67.14089408960525}}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_results[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'clean_loss': 63.91732102791386,\n",
       "  'corrupted_loss': 132.8242889027916,\n",
       "  'restored_loss': {'transformer.h.0.attn': 63.91732102791386,\n",
       "   'transformer.h.0.mlp': 72.45838449838195,\n",
       "   'transformer.h.1.attn': 108.48160917896826,\n",
       "   'transformer.h.1.mlp': 98.25952101895331,\n",
       "   'transformer.h.2.attn': 105.73393221716806,\n",
       "   'transformer.h.2.mlp': 157.6101965283722,\n",
       "   'transformer.h.3.attn': 114.146045922409,\n",
       "   'transformer.h.3.mlp': 115.46617860310457,\n",
       "   'transformer.h.4.attn': 100.84806083556465,\n",
       "   'transformer.h.4.mlp': 95.89193990231547,\n",
       "   'transformer.h.5.attn': 131.30281967082848,\n",
       "   'transformer.h.5.mlp': 97.5344761810575,\n",
       "   'transformer.h.6.attn': 102.20784520792095,\n",
       "   'transformer.h.6.mlp': 109.59660862325092,\n",
       "   'transformer.h.7.attn': 106.16173384104698,\n",
       "   'transformer.h.7.mlp': 95.2186388402459,\n",
       "   'transformer.h.8.attn': 112.70963296738027,\n",
       "   'transformer.h.8.mlp': 108.70634142756694,\n",
       "   'transformer.h.9.attn': 127.1640592432581,\n",
       "   'transformer.h.9.mlp': 84.25118249435614,\n",
       "   'transformer.h.10.attn': 125.37441122644063,\n",
       "   'transformer.h.10.mlp': 118.2573743814645,\n",
       "   'transformer.h.11.attn': 115.2849580506915,\n",
       "   'transformer.h.11.mlp': 117.34240362818905}},\n",
       " {'clean_loss': 33.48678344461458,\n",
       "  'corrupted_loss': 60.98568078003749,\n",
       "  'restored_loss': {'transformer.h.0.attn': 33.48678344461458,\n",
       "   'transformer.h.0.mlp': 37.003198415471566,\n",
       "   'transformer.h.1.attn': 64.097670397889,\n",
       "   'transformer.h.1.mlp': 72.50092908024622,\n",
       "   'transformer.h.2.attn': 48.12944801536147,\n",
       "   'transformer.h.2.mlp': 55.87564014050752,\n",
       "   'transformer.h.3.attn': 49.198414002287066,\n",
       "   'transformer.h.3.mlp': 56.622639772611144,\n",
       "   'transformer.h.4.attn': 52.96746860053392,\n",
       "   'transformer.h.4.mlp': 58.081943900922326,\n",
       "   'transformer.h.5.attn': 47.15663012977383,\n",
       "   'transformer.h.5.mlp': 58.93682358602892,\n",
       "   'transformer.h.6.attn': 50.619899921520926,\n",
       "   'transformer.h.6.mlp': 52.43684580887924,\n",
       "   'transformer.h.7.attn': 49.19644343267088,\n",
       "   'transformer.h.7.mlp': 48.81173161719443,\n",
       "   'transformer.h.8.attn': 49.08029601688048,\n",
       "   'transformer.h.8.mlp': 52.709431077000666,\n",
       "   'transformer.h.9.attn': 53.61403110579979,\n",
       "   'transformer.h.9.mlp': 48.12371088002379,\n",
       "   'transformer.h.10.attn': 55.51620696007936,\n",
       "   'transformer.h.10.mlp': 49.97331926353094,\n",
       "   'transformer.h.11.attn': 57.22955743146801,\n",
       "   'transformer.h.11.mlp': 54.953368785640976}},\n",
       " {'clean_loss': 14.614322888160343,\n",
       "  'corrupted_loss': 178.84399700976167,\n",
       "  'restored_loss': {'transformer.h.0.attn': 14.614322888160343,\n",
       "   'transformer.h.0.mlp': 24.60711412079997,\n",
       "   'transformer.h.1.attn': 222.1300657441556,\n",
       "   'transformer.h.1.mlp': 107.46851631638697,\n",
       "   'transformer.h.2.attn': 137.68372165579638,\n",
       "   'transformer.h.2.mlp': 202.31722056913154,\n",
       "   'transformer.h.3.attn': 74.8837955315738,\n",
       "   'transformer.h.3.mlp': 116.30228236906993,\n",
       "   'transformer.h.4.attn': 102.4402409559173,\n",
       "   'transformer.h.4.mlp': 112.37750080436398,\n",
       "   'transformer.h.5.attn': 95.64849033743964,\n",
       "   'transformer.h.5.mlp': 84.30201805425571,\n",
       "   'transformer.h.6.attn': 124.62934418060337,\n",
       "   'transformer.h.6.mlp': 75.63842723730501,\n",
       "   'transformer.h.7.attn': 162.58746243439813,\n",
       "   'transformer.h.7.mlp': 79.85231997278244,\n",
       "   'transformer.h.8.attn': 140.8238425421443,\n",
       "   'transformer.h.8.mlp': 107.7130789954823,\n",
       "   'transformer.h.9.attn': 165.1277627333633,\n",
       "   'transformer.h.9.mlp': 128.27489363427617,\n",
       "   'transformer.h.10.attn': 170.65087917703588,\n",
       "   'transformer.h.10.mlp': 66.52228107764671,\n",
       "   'transformer.h.11.attn': 147.28270691984204,\n",
       "   'transformer.h.11.mlp': 67.14089408960525}}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_results[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TMOD Two models with one data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First run: tuned run def\n",
    "def save_tuned_activation(m_id):\n",
    "    def save_tuned_activation_hook(module, _input, _output):\n",
    "#         print(m_id, _output.shape)\n",
    "        if m_id.endswith('attn'):\n",
    "            tuned_activations[m_id] = _output[0].detach()\n",
    "        elif m_id.endswith('mlp'):\n",
    "#         else:\n",
    "            tuned_activations[m_id] = _output.detach()\n",
    "    return save_tuned_activation_hook\n",
    "\n",
    "tuned_model = load_model()\n",
    "tuned_model.eval()\n",
    "for m_id in list_trace_module_ids:\n",
    "    tuned_model.get_submodule(m_id).register_forward_hook(save_tuned_activation(m_id))    \n",
    "\n",
    "    \n",
    "# Second run: base run def    \n",
    "# def corrupt_input_vector(module, _input):#, _output):\n",
    "#     torch.manual_seed(718)\n",
    "#     std = torch.std(_input[0])\n",
    "#     return tuple([_input[0] + (std*1.5) * torch.randn(_input[0].shape).to(device_id), ])\n",
    "\n",
    "base_model = GPT2LMHeadModel.from_pretrained(\"gpt2\", cache_dir=cache_dir).to(device_id)\n",
    "base_model.eval()\n",
    "# base_model.get_submodule(\"transformer.h.0.attn\").register_forward_pre_hook(corrupt_input_vector)    \n",
    "\n",
    "\n",
    "# Third run: restored run def    \n",
    "def restore_activation(m_id):\n",
    "    def restore_activation_hook(module, _input, _output):\n",
    "        tuned_activation = tuned_activations[m_id]#[:, t]\n",
    "        if m_id.endswith('attn'):\n",
    "            return tuple([tuned_activation, tuple([_output[1][0], _output[1][1]])])\n",
    "        elif m_id.endswith('mlp'):           \n",
    "            base_output = _output.detach()\n",
    "            base_output = tuned_activation\n",
    "            return base_output\n",
    "    return restore_activation_hook\n",
    "\n",
    "restored_model = GPT2LMHeadModel.from_pretrained(\"gpt2\", cache_dir=cache_dir).to(device_id)\n",
    "restored_model.eval()\n",
    "\n",
    "# restored_model.get_submodule(\"transformer.h.0.attn\").register_forward_pre_hook(corrupt_input_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causal Tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(output_file, 'r') as json_file:\n",
    "        list_results = json.load(json_file)\n",
    "    for i, d in enumerate(list_results):\n",
    "        if len(d) == 0:\n",
    "            done_idx = i - 1\n",
    "            print(done_idx)\n",
    "            break\n",
    "except:\n",
    "    list_results = [{} for x in range(len_sentences)]\n",
    "    done_idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence_idx: 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m m_id \u001b[38;5;129;01min\u001b[39;00m list_trace_module_ids:\n\u001b[1;32m     28\u001b[0m     hook \u001b[38;5;241m=\u001b[39m restored_model\u001b[38;5;241m.\u001b[39mget_submodule(m_id)\u001b[38;5;241m.\u001b[39mregister_forward_hook(restore_activation(m_id))\n\u001b[0;32m---> 29\u001b[0m     restored_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mrestored_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     restored_loss[m_id] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(restored_outputs\u001b[38;5;241m.\u001b[39mloss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     31\u001b[0m     hook\u001b[38;5;241m.\u001b[39mremove()\n",
      "File \u001b[0;32m~/anaconda3/envs/m2d2/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/projects/m2d2/transformers/src/transformers/models/gpt2/modeling_gpt2.py:1076\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1068\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1072\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1073\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1074\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1076\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1081\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1082\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1083\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1084\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1085\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1086\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1087\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1088\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1089\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1090\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1091\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1093\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/m2d2/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/projects/m2d2/transformers/src/transformers/models/gpt2/modeling_gpt2.py:900\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    890\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    891\u001b[0m         create_custom_forward(block),\n\u001b[1;32m    892\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    897\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    898\u001b[0m     )\n\u001b[1;32m    899\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 900\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    904\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    906\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    907\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    908\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    909\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    911\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    912\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/m2d2/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/projects/m2d2/transformers/src/transformers/models/gpt2/modeling_gpt2.py:390\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    388\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    389\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_1(hidden_states)\n\u001b[0;32m--> 390\u001b[0m attn_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    398\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[1;32m    399\u001b[0m outputs \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[0;32m~/anaconda3/envs/m2d2/lib/python3.10/site-packages/torch/nn/modules/module.py:1538\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1535\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1536\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1538\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1540\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1541\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1542\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1543\u001b[0m     ):\n",
      "File \u001b[0;32m~/projects/m2d2/transformers/src/transformers/models/gpt2/modeling_gpt2.py:334\u001b[0m, in \u001b[0;36mGPT2Attention.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    331\u001b[0m     attn_output, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_attn(query, key, value, attention_mask, head_mask)\n\u001b[1;32m    333\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_heads(attn_output, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n\u001b[0;32m--> 334\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    335\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresid_dropout(attn_output)\n\u001b[1;32m    337\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (attn_output, present)\n",
      "File \u001b[0;32m~/anaconda3/envs/m2d2/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/projects/m2d2/transformers/src/transformers/pytorch_utils.py:102\u001b[0m, in \u001b[0;36mConv1D.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    101\u001b[0m     size_out \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnf,)\n\u001b[0;32m--> 102\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddmm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(size_out)\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.manual_seed(718)\n",
    "set_seed(718)\n",
    "\n",
    "for sentence_idx, data in enumerate(tokenized_datasets):\n",
    "    if done_idx > sentence_idx: continue\n",
    "    if sentence_idx % 1000 == 0:\n",
    "        print(f\"sentence_idx: {sentence_idx}\")\n",
    "        with open(output_file, 'w') as json_file:\n",
    "            json.dump(list_results, json_file)\n",
    "    \n",
    "    inputs = torch.tensor(data['input_ids']).to(device_id)\n",
    "    \n",
    "    # First run: tuned run        \n",
    "    tuned_activations = {}\n",
    "    with torch.no_grad():\n",
    "        tuned_outputs = tuned_model(inputs, labels=inputs.clone())\n",
    "        tuned_loss = np.exp(tuned_outputs.loss.item())\n",
    "\n",
    "    # Second run: base run    \n",
    "    with torch.no_grad():\n",
    "        base_outputs = base_model(inputs, labels=inputs.clone())\n",
    "        base_loss = np.exp(base_outputs.loss.item())\n",
    "        \n",
    "    # Third run: base-with-restoration run    \n",
    "    restored_loss = {}\n",
    "    with torch.no_grad():\n",
    "        for m_id in list_trace_module_ids:\n",
    "            hook = restored_model.get_submodule(m_id).register_forward_hook(restore_activation(m_id))\n",
    "            restored_outputs = restored_model(inputs, labels=inputs.clone())\n",
    "            restored_loss[m_id] = np.exp(restored_outputs.loss.item())\n",
    "            hook.remove()\n",
    "            \n",
    "    list_results[sentence_idx]['tuned_loss'] = tuned_loss\n",
    "    list_results[sentence_idx]['base_loss'] = base_loss\n",
    "    list_results[sentence_idx]['restored_loss'] = restored_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:m2d2]",
   "language": "python",
   "name": "conda-env-m2d2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
