{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rds/general/user/jj1122/home/anaconda3/envs/m2d2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast, GPT2Tokenizer\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import torch\n",
    "import argparse\n",
    "import datasets\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "from os import listdir\n",
    "import json\n",
    "import math\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = 12\n",
    "list_modules = ['attn', 'mlp']\n",
    "trace_module_id = \"transformer.h.{l}.{m}\"\n",
    "\n",
    "list_trace_module_ids = []\n",
    "\n",
    "for l in range(n_layers):\n",
    "    for m in list_modules:\n",
    "        list_trace_module_ids.append(trace_module_id.format(l=l, m=m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2 = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "gpt2_tokeniser = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    output = gpt2_tokeniser(examples)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = {}\n",
    "\n",
    "def func_v1(m_id):\n",
    "    def hook_v1(module, _input, _output):\n",
    "        global seq\n",
    "        dict_sequence[m_id].append(seq)\n",
    "        seq += 1\n",
    "        \n",
    "        module_ = list(module) if isinstance(module, tuple) else module\n",
    "        input_ = list(_input) if isinstance(_input, tuple) else _input\n",
    "        output_ = list(_output) if isinstance(_output, tuple) else _output\n",
    "#         print(type(module_), module_)\n",
    "        temp[m_id] = [module_, input_, output_]\n",
    "        \n",
    "    return hook_v1\n",
    "\n",
    "gpt2 = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "for m_id in list_trace_module_ids:\n",
    "    gpt2.get_submodule(m_id).register_forward_hook(func_v1(m_id))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5303, 23748],\n",
       "        [11274, 33847]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = [\"hi hello\", 'good bye']\n",
    "data = tokenize_function(input_text)\n",
    "inputs = torch.tensor(data['input_ids'])\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_sequence = {x: [] for x in list_trace_module_ids}\n",
    "seq = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    clean_outputs = gpt2(inputs, labels=inputs.clone())\n",
    "    clean_loss = np.exp(clean_outputs.loss.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'transformer.h.0.attn': [0],\n",
       " 'transformer.h.0.mlp': [1],\n",
       " 'transformer.h.1.attn': [2],\n",
       " 'transformer.h.1.mlp': [3],\n",
       " 'transformer.h.2.attn': [4],\n",
       " 'transformer.h.2.mlp': [5],\n",
       " 'transformer.h.3.attn': [6],\n",
       " 'transformer.h.3.mlp': [7],\n",
       " 'transformer.h.4.attn': [8],\n",
       " 'transformer.h.4.mlp': [9],\n",
       " 'transformer.h.5.attn': [10],\n",
       " 'transformer.h.5.mlp': [11],\n",
       " 'transformer.h.6.attn': [12],\n",
       " 'transformer.h.6.mlp': [13],\n",
       " 'transformer.h.7.attn': [14],\n",
       " 'transformer.h.7.mlp': [15],\n",
       " 'transformer.h.8.attn': [16],\n",
       " 'transformer.h.8.mlp': [17],\n",
       " 'transformer.h.9.attn': [18],\n",
       " 'transformer.h.9.mlp': [19],\n",
       " 'transformer.h.10.attn': [20],\n",
       " 'transformer.h.10.mlp': [21],\n",
       " 'transformer.h.11.attn': [22],\n",
       " 'transformer.h.11.mlp': [23]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ATTN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Attention(\n",
       "  (c_attn): Conv1D()\n",
       "  (c_proj): Conv1D()\n",
       "  (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "  (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp['transformer.h.2.attn'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(temp['transformer.h.2.attn'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 768])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp['transformer.h.2.attn'][1][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### output (activation, (k, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(temp['transformer.h.2.attn'][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### activation $ \\in \\mathbb{R}^{n\\_batch \\times n\\_tokens \\times dim\\_model}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 768])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp['transformer.h.2.attn'][2][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k, v $ \\in \\mathbb{R}^{n\\_batch \\times n\\_heads \\times n\\_tokens \\times dim\\_head}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 12, 2, 64])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp['transformer.h.2.attn'][2][1][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2MLP(\n",
       "  (c_fc): Conv1D()\n",
       "  (c_proj): Conv1D()\n",
       "  (act): NewGELUActivation()\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp['transformer.h.0.mlp'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### input (n_tokens, dim_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(temp['transformer.h.0.mlp'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 768])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp['transformer.h.0.mlp'][1][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### output (n_tokens, dim_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(temp['transformer.h.0.mlp'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 768])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp['transformer.h.0.mlp'][2][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1473, -0.8551,  0.2517,  ..., -1.0792,  0.4386, -0.8061],\n",
       "        [-0.9995, -0.4823, -1.1081,  ..., -0.7381, -0.0529, -1.1930]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp['transformer.h.0.mlp'][2][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "m2d2_env",
   "language": "python",
   "name": "m2d2_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
