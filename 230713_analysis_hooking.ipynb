{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 230713_analysis_hooking\n",
    "각 submodule의 input output을 조사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rds/general/user/jj1122/home/anaconda3/envs/m2d2_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast, GPT2Tokenizer\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import torch\n",
    "import argparse\n",
    "import datasets\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "from os import listdir\n",
    "import json\n",
    "import math\n",
    "import seaborn as sns "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = 12\n",
    "list_modules = ['attn', 'mlp']\n",
    "trace_module_id = \"transformer.h.{l}.{m}\"\n",
    "\n",
    "list_trace_module_ids = []\n",
    "\n",
    "for l in range(n_layers):\n",
    "    for m in list_modules:\n",
    "        list_trace_module_ids.append(trace_module_id.format(l=l, m=m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_trace_module_ids = [\n",
    "    'transformer.h.0.attn.c_attn', 'transformer.h.0.attn.c_proj',\n",
    "    'transformer.h.0.mlp.c_fc', 'transformer.h.0.mlp.c_proj',\n",
    "    'transformer.h.1.attn.c_attn', 'transformer.h.1.attn.c_proj',\n",
    "    'transformer.h.1.mlp.c_fc', 'transformer.h.1.mlp.c_proj',\n",
    "    'transformer.h.2.attn.c_attn', 'transformer.h.2.attn.c_proj',\n",
    "    'transformer.h.2.mlp.c_fc', 'transformer.h.2.mlp.c_proj',\n",
    "    'transformer.h.3.attn.c_attn', 'transformer.h.3.attn.c_proj',\n",
    "    'transformer.h.3.mlp.c_fc', 'transformer.h.3.mlp.c_proj',\n",
    "    'transformer.h.4.attn.c_attn', 'transformer.h.4.attn.c_proj',\n",
    "    'transformer.h.4.mlp.c_fc', 'transformer.h.4.mlp.c_proj',\n",
    "    'transformer.h.5.attn.c_attn', 'transformer.h.5.attn.c_proj',\n",
    "    'transformer.h.5.mlp.c_fc', 'transformer.h.5.mlp.c_proj',\n",
    "    'transformer.h.6.attn.c_attn', 'transformer.h.6.attn.c_proj',\n",
    "    'transformer.h.6.mlp.c_fc', 'transformer.h.6.mlp.c_proj',\n",
    "    'transformer.h.7.attn.c_attn', 'transformer.h.7.attn.c_proj',\n",
    "    'transformer.h.7.mlp.c_fc', 'transformer.h.7.mlp.c_proj',\n",
    "    'transformer.h.8.attn.c_attn', 'transformer.h.8.attn.c_proj',\n",
    "    'transformer.h.8.mlp.c_fc', 'transformer.h.8.mlp.c_proj',\n",
    "    'transformer.h.9.attn.c_attn', 'transformer.h.9.attn.c_proj',\n",
    "    'transformer.h.9.mlp.c_fc', 'transformer.h.9.mlp.c_proj',\n",
    "    'transformer.h.10.attn.c_attn', 'transformer.h.10.attn.c_proj',\n",
    "    'transformer.h.10.mlp.c_fc', 'transformer.h.10.mlp.c_proj',\n",
    "    'transformer.h.11.attn.c_attn', 'transformer.h.11.attn.c_proj',\n",
    "    'transformer.h.11.mlp.c_fc', 'transformer.h.11.mlp.c_proj'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "transformer\n",
      "transformer.wte\n",
      "transformer.wpe\n",
      "transformer.drop\n",
      "transformer.h\n",
      "transformer.h.0\n",
      "transformer.h.0.ln_1\n",
      "transformer.h.0.attn\n",
      "transformer.h.0.attn.c_attn\n",
      "transformer.h.0.attn.c_proj\n",
      "transformer.h.0.attn.attn_dropout\n",
      "transformer.h.0.attn.resid_dropout\n",
      "transformer.h.0.ln_2\n",
      "transformer.h.0.mlp\n",
      "transformer.h.0.mlp.c_fc\n",
      "transformer.h.0.mlp.c_proj\n",
      "transformer.h.0.mlp.act\n",
      "transformer.h.0.mlp.dropout\n",
      "transformer.h.1\n",
      "transformer.h.1.ln_1\n",
      "transformer.h.1.attn\n",
      "transformer.h.1.attn.c_attn\n",
      "transformer.h.1.attn.c_proj\n",
      "transformer.h.1.attn.attn_dropout\n",
      "transformer.h.1.attn.resid_dropout\n",
      "transformer.h.1.ln_2\n",
      "transformer.h.1.mlp\n",
      "transformer.h.1.mlp.c_fc\n",
      "transformer.h.1.mlp.c_proj\n",
      "transformer.h.1.mlp.act\n",
      "transformer.h.1.mlp.dropout\n",
      "transformer.h.2\n",
      "transformer.h.2.ln_1\n",
      "transformer.h.2.attn\n",
      "transformer.h.2.attn.c_attn\n",
      "transformer.h.2.attn.c_proj\n",
      "transformer.h.2.attn.attn_dropout\n",
      "transformer.h.2.attn.resid_dropout\n",
      "transformer.h.2.ln_2\n",
      "transformer.h.2.mlp\n",
      "transformer.h.2.mlp.c_fc\n",
      "transformer.h.2.mlp.c_proj\n",
      "transformer.h.2.mlp.act\n",
      "transformer.h.2.mlp.dropout\n",
      "transformer.h.3\n",
      "transformer.h.3.ln_1\n",
      "transformer.h.3.attn\n",
      "transformer.h.3.attn.c_attn\n",
      "transformer.h.3.attn.c_proj\n",
      "transformer.h.3.attn.attn_dropout\n",
      "transformer.h.3.attn.resid_dropout\n",
      "transformer.h.3.ln_2\n",
      "transformer.h.3.mlp\n",
      "transformer.h.3.mlp.c_fc\n",
      "transformer.h.3.mlp.c_proj\n",
      "transformer.h.3.mlp.act\n",
      "transformer.h.3.mlp.dropout\n",
      "transformer.h.4\n",
      "transformer.h.4.ln_1\n",
      "transformer.h.4.attn\n",
      "transformer.h.4.attn.c_attn\n",
      "transformer.h.4.attn.c_proj\n",
      "transformer.h.4.attn.attn_dropout\n",
      "transformer.h.4.attn.resid_dropout\n",
      "transformer.h.4.ln_2\n",
      "transformer.h.4.mlp\n",
      "transformer.h.4.mlp.c_fc\n",
      "transformer.h.4.mlp.c_proj\n",
      "transformer.h.4.mlp.act\n",
      "transformer.h.4.mlp.dropout\n",
      "transformer.h.5\n",
      "transformer.h.5.ln_1\n",
      "transformer.h.5.attn\n",
      "transformer.h.5.attn.c_attn\n",
      "transformer.h.5.attn.c_proj\n",
      "transformer.h.5.attn.attn_dropout\n",
      "transformer.h.5.attn.resid_dropout\n",
      "transformer.h.5.ln_2\n",
      "transformer.h.5.mlp\n",
      "transformer.h.5.mlp.c_fc\n",
      "transformer.h.5.mlp.c_proj\n",
      "transformer.h.5.mlp.act\n",
      "transformer.h.5.mlp.dropout\n",
      "transformer.h.6\n",
      "transformer.h.6.ln_1\n",
      "transformer.h.6.attn\n",
      "transformer.h.6.attn.c_attn\n",
      "transformer.h.6.attn.c_proj\n",
      "transformer.h.6.attn.attn_dropout\n",
      "transformer.h.6.attn.resid_dropout\n",
      "transformer.h.6.ln_2\n",
      "transformer.h.6.mlp\n",
      "transformer.h.6.mlp.c_fc\n",
      "transformer.h.6.mlp.c_proj\n",
      "transformer.h.6.mlp.act\n",
      "transformer.h.6.mlp.dropout\n",
      "transformer.h.7\n",
      "transformer.h.7.ln_1\n",
      "transformer.h.7.attn\n",
      "transformer.h.7.attn.c_attn\n",
      "transformer.h.7.attn.c_proj\n",
      "transformer.h.7.attn.attn_dropout\n",
      "transformer.h.7.attn.resid_dropout\n",
      "transformer.h.7.ln_2\n",
      "transformer.h.7.mlp\n",
      "transformer.h.7.mlp.c_fc\n",
      "transformer.h.7.mlp.c_proj\n",
      "transformer.h.7.mlp.act\n",
      "transformer.h.7.mlp.dropout\n",
      "transformer.h.8\n",
      "transformer.h.8.ln_1\n",
      "transformer.h.8.attn\n",
      "transformer.h.8.attn.c_attn\n",
      "transformer.h.8.attn.c_proj\n",
      "transformer.h.8.attn.attn_dropout\n",
      "transformer.h.8.attn.resid_dropout\n",
      "transformer.h.8.ln_2\n",
      "transformer.h.8.mlp\n",
      "transformer.h.8.mlp.c_fc\n",
      "transformer.h.8.mlp.c_proj\n",
      "transformer.h.8.mlp.act\n",
      "transformer.h.8.mlp.dropout\n",
      "transformer.h.9\n",
      "transformer.h.9.ln_1\n",
      "transformer.h.9.attn\n",
      "transformer.h.9.attn.c_attn\n",
      "transformer.h.9.attn.c_proj\n",
      "transformer.h.9.attn.attn_dropout\n",
      "transformer.h.9.attn.resid_dropout\n",
      "transformer.h.9.ln_2\n",
      "transformer.h.9.mlp\n",
      "transformer.h.9.mlp.c_fc\n",
      "transformer.h.9.mlp.c_proj\n",
      "transformer.h.9.mlp.act\n",
      "transformer.h.9.mlp.dropout\n",
      "transformer.h.10\n",
      "transformer.h.10.ln_1\n",
      "transformer.h.10.attn\n",
      "transformer.h.10.attn.c_attn\n",
      "transformer.h.10.attn.c_proj\n",
      "transformer.h.10.attn.attn_dropout\n",
      "transformer.h.10.attn.resid_dropout\n",
      "transformer.h.10.ln_2\n",
      "transformer.h.10.mlp\n",
      "transformer.h.10.mlp.c_fc\n",
      "transformer.h.10.mlp.c_proj\n",
      "transformer.h.10.mlp.act\n",
      "transformer.h.10.mlp.dropout\n",
      "transformer.h.11\n",
      "transformer.h.11.ln_1\n",
      "transformer.h.11.attn\n",
      "transformer.h.11.attn.c_attn\n",
      "transformer.h.11.attn.c_proj\n",
      "transformer.h.11.attn.attn_dropout\n",
      "transformer.h.11.attn.resid_dropout\n",
      "transformer.h.11.ln_2\n",
      "transformer.h.11.mlp\n",
      "transformer.h.11.mlp.c_fc\n",
      "transformer.h.11.mlp.c_proj\n",
      "transformer.h.11.mlp.act\n",
      "transformer.h.11.mlp.dropout\n",
      "transformer.ln_f\n",
      "lm_head\n"
     ]
    }
   ],
   "source": [
    "for tup in gpt2.named_modules():\n",
    "    print(tup[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2 = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "gpt2_tokeniser = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    output = gpt2_tokeniser(examples)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = {}\n",
    "\n",
    "def func_v1(m_id):\n",
    "    def hook_v1(module, _input, _output):\n",
    "        global seq\n",
    "        dict_sequence[m_id].append(seq)\n",
    "        seq += 1\n",
    "        \n",
    "        module_ = list(module) if isinstance(module, tuple) else module\n",
    "        input_ = list(_input) if isinstance(_input, tuple) else _input\n",
    "        output_ = list(_output) if isinstance(_output, tuple) else _output\n",
    "#         print(type(module_), module_)\n",
    "        temp[m_id] = [module_, input_, output_]\n",
    "        \n",
    "    return hook_v1\n",
    "\n",
    "gpt2 = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "for m_id in list_trace_module_ids:\n",
    "    gpt2.get_submodule(m_id).register_forward_hook(func_v1(m_id))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5303, 23748,   345],\n",
       "        [11274, 33847,   345]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = [\"hi hello you\", 'good bye you']\n",
    "data = tokenize_function(input_text)\n",
    "inputs = torch.tensor(data['input_ids'])\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_sequence = {x: [] for x in list_trace_module_ids}\n",
    "seq = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    clean_outputs = gpt2(inputs, labels=inputs.clone())\n",
    "    clean_loss = np.exp(clean_outputs.loss.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'transformer.h.0.attn.c_attn': [0],\n",
       " 'transformer.h.0.attn.c_proj': [1],\n",
       " 'transformer.h.0.mlp.c_fc': [2],\n",
       " 'transformer.h.0.mlp.c_proj': [3],\n",
       " 'transformer.h.1.attn.c_attn': [4],\n",
       " 'transformer.h.1.attn.c_proj': [5],\n",
       " 'transformer.h.1.mlp.c_fc': [6],\n",
       " 'transformer.h.1.mlp.c_proj': [7],\n",
       " 'transformer.h.2.attn.c_attn': [8],\n",
       " 'transformer.h.2.attn.c_proj': [9],\n",
       " 'transformer.h.2.mlp.c_fc': [10],\n",
       " 'transformer.h.2.mlp.c_proj': [11],\n",
       " 'transformer.h.3.attn.c_attn': [12],\n",
       " 'transformer.h.3.attn.c_proj': [13],\n",
       " 'transformer.h.3.mlp.c_fc': [14],\n",
       " 'transformer.h.3.mlp.c_proj': [15],\n",
       " 'transformer.h.4.attn.c_attn': [16],\n",
       " 'transformer.h.4.attn.c_proj': [17],\n",
       " 'transformer.h.4.mlp.c_fc': [18],\n",
       " 'transformer.h.4.mlp.c_proj': [19],\n",
       " 'transformer.h.5.attn.c_attn': [20],\n",
       " 'transformer.h.5.attn.c_proj': [21],\n",
       " 'transformer.h.5.mlp.c_fc': [22],\n",
       " 'transformer.h.5.mlp.c_proj': [23],\n",
       " 'transformer.h.6.attn.c_attn': [24],\n",
       " 'transformer.h.6.attn.c_proj': [25],\n",
       " 'transformer.h.6.mlp.c_fc': [26],\n",
       " 'transformer.h.6.mlp.c_proj': [27],\n",
       " 'transformer.h.7.attn.c_attn': [28],\n",
       " 'transformer.h.7.attn.c_proj': [29],\n",
       " 'transformer.h.7.mlp.c_fc': [30],\n",
       " 'transformer.h.7.mlp.c_proj': [31],\n",
       " 'transformer.h.8.attn.c_attn': [32],\n",
       " 'transformer.h.8.attn.c_proj': [33],\n",
       " 'transformer.h.8.mlp.c_fc': [34],\n",
       " 'transformer.h.8.mlp.c_proj': [35],\n",
       " 'transformer.h.9.attn.c_attn': [36],\n",
       " 'transformer.h.9.attn.c_proj': [37],\n",
       " 'transformer.h.9.mlp.c_fc': [38],\n",
       " 'transformer.h.9.mlp.c_proj': [39],\n",
       " 'transformer.h.10.attn.c_attn': [40],\n",
       " 'transformer.h.10.attn.c_proj': [41],\n",
       " 'transformer.h.10.mlp.c_fc': [42],\n",
       " 'transformer.h.10.mlp.c_proj': [43],\n",
       " 'transformer.h.11.attn.c_attn': [44],\n",
       " 'transformer.h.11.attn.c_proj': [45],\n",
       " 'transformer.h.11.mlp.c_fc': [46],\n",
       " 'transformer.h.11.mlp.c_proj': [47]}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ATTN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## attn_c_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_c_attn = \"transformer.h.0.attn.c_attn\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv1D()"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp[attn_c_attn][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([768, 2304]), torch.Size([2304]))"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp[attn_c_attn][0].weight.shape, temp[attn_c_attn][0].bias.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[-0.1000, -0.0874, -0.0269,  ..., -0.0154,  0.0138,  0.1415],\n",
       "          [ 0.1052, -0.1196, -0.0540,  ..., -0.1193,  0.0812,  0.0517],\n",
       "          [-0.0343, -0.0042,  0.0058,  ..., -0.1106, -0.0695, -0.0866]],\n",
       " \n",
       "         [[ 0.0065, -0.0714, -0.0597,  ..., -0.0201, -0.0945, -0.0281],\n",
       "          [ 0.0837, -0.2269, -0.0595,  ..., -0.3107, -0.1329,  0.0581],\n",
       "          [-0.0343, -0.0042,  0.0058,  ..., -0.1106, -0.0695, -0.0866]]])]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp[attn_c_attn][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 768])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp[attn_c_attn][1][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### output (activation, (k, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.6655, -0.2217,  0.1277,  ...,  0.0662, -0.0129,  0.0362],\n",
       "         [ 0.4013,  0.8140,  0.4187,  ..., -0.0534, -0.0017,  0.3631],\n",
       "         [-0.0061, -0.6571, -0.9560,  ...,  0.0841,  0.1047,  0.2007]],\n",
       "\n",
       "        [[-0.0903, -0.4669, -0.4292,  ..., -0.0223, -0.0383,  0.0397],\n",
       "         [-0.0209,  0.3601, -0.9751,  ...,  0.0634, -0.1967,  0.4776],\n",
       "         [-0.0061, -0.6571, -0.9560,  ...,  0.0841,  0.1047,  0.2007]]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp[attn_c_attn][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 2304])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp[attn_c_attn][2].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## attn_c_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_c_proj = \"transformer.h.0.attn.c_proj\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([768, 768]), torch.Size([768]))"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp[attn_c_proj][0].weight.shape, temp[attn_c_proj][0].bias.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[ 0.1432, -0.0883, -0.1866,  ...,  0.0662, -0.0129,  0.0362],\n",
       "          [ 0.1754, -0.0284, -0.1838,  ..., -0.0015, -0.0066,  0.2211],\n",
       "          [ 0.1662,  0.0007, -0.1727,  ...,  0.0411,  0.0261,  0.1704]],\n",
       " \n",
       "         [[ 0.1572, -0.0891,  0.0698,  ..., -0.0223, -0.0383,  0.0397],\n",
       "          [ 0.1471, -0.0393,  0.1088,  ...,  0.0210, -0.1183,  0.2609],\n",
       "          [ 0.1256,  0.0327,  0.1329,  ...,  0.0261, -0.0372,  0.1851]]])]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp[attn_c_proj][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 768])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp[attn_c_proj][1][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### output (activation, (k, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.1389, -1.3163, -0.0550,  ...,  0.0416,  0.0390, -0.0281],\n",
       "         [ 0.7931, -0.4850, -0.4926,  ...,  0.0448,  0.0371, -0.0997],\n",
       "         [-0.1901,  0.0686, -1.0865,  ...,  0.0132,  0.0369,  0.0170]],\n",
       "\n",
       "        [[ 0.6993, -1.0576,  0.3251,  ...,  0.0449, -0.0214,  0.0940],\n",
       "         [ 2.2979, -1.0934, -0.0447,  ...,  0.0073, -0.0284,  0.0597],\n",
       "         [-0.1887, -0.4862, -0.6547,  ..., -0.0183, -0.0460, -0.0433]]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp[attn_c_proj][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 768])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp[attn_c_proj][2].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mlp_c_fc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_c_fc = \"transformer.h.0.mlp.c_fc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv1D()"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp[mlp_c_fc][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([768, 3072]), torch.Size([3072]))"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp[mlp_c_fc][0].weight.shape, temp[mlp_c_fc][0].bias.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[ 1.4949e-01, -2.3579e-01,  1.3381e-02,  ...,  3.1491e-02,\n",
       "            1.2534e-01,  2.7030e-01],\n",
       "          [ 1.2421e-01, -6.7010e-02, -6.6431e-02,  ..., -1.0497e-01,\n",
       "            1.3507e-01, -2.5451e-02],\n",
       "          [ 1.5652e-02,  3.4081e-02, -1.7358e-01,  ..., -1.4329e-01,\n",
       "           -4.6414e-02, -7.6008e-02]],\n",
       " \n",
       "         [[ 1.2513e-01, -1.9504e-01,  6.6614e-02,  ...,  2.4091e-02,\n",
       "           -1.9277e-01,  7.2787e-02],\n",
       "          [ 2.5648e-01, -1.7055e-01, -7.6198e-05,  ..., -3.4101e-01,\n",
       "           -1.5102e-01,  1.1415e-01],\n",
       "          [ 1.5357e-02, -6.5804e-02, -1.0100e-01,  ..., -1.7909e-01,\n",
       "           -1.3713e-01, -1.3167e-01]]])]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp[mlp_c_fc][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 768])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp[mlp_c_fc][1][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4491, -1.5715, -1.4653,  ..., -2.0566, -2.0355,  0.6436],\n",
       "         [ 0.6902,  0.1128,  0.2437,  ..., -1.0926, -1.4637, -0.1455],\n",
       "         [ 0.5122, -1.0178, -0.7147,  ..., -1.7263, -0.9549, -0.0196]],\n",
       "\n",
       "        [[-0.0673, -1.6470, -1.0625,  ..., -1.9722, -1.9229,  0.3057],\n",
       "         [ 0.5578,  0.2786, -0.1493,  ..., -1.3881, -0.5546,  0.1224],\n",
       "         [ 0.4659, -1.1941, -0.6374,  ..., -1.5148, -0.9111, -0.2381]]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp[mlp_c_fc][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 3072])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp[mlp_c_fc][2].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mlp_c_proj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_c_proj = \"transformer.h.0.mlp.c_proj\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv1D()"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp[mlp_c_proj][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3072, 768]), torch.Size([768]))"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp[mlp_c_proj][0].weight.shape, temp[mlp_c_proj][0].bias.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[ 0.3024, -0.0914, -0.1049,  ..., -0.0407, -0.0424,  0.4763],\n",
       "          [ 0.5210,  0.0614,  0.1453,  ..., -0.1502, -0.1051, -0.0643],\n",
       "          [ 0.3564, -0.1573, -0.1697,  ..., -0.0729, -0.1623, -0.0096]],\n",
       " \n",
       "         [[-0.0318, -0.0821, -0.1532,  ..., -0.0478, -0.0524,  0.1896],\n",
       "          [ 0.3968,  0.1699, -0.0658,  ..., -0.1148, -0.1606,  0.0672],\n",
       "          [ 0.3165, -0.1390, -0.1670,  ..., -0.0985, -0.1651, -0.0966]]])]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp[mlp_c_proj][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 3072])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp[mlp_c_proj][1][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1473, -0.8551,  0.2517,  ..., -1.0792,  0.4386, -0.8061],\n",
       "         [-0.9995, -0.4823, -1.1081,  ..., -0.7381, -0.0529, -1.1930],\n",
       "         [-0.9196,  0.3858, -0.1840,  ..., -0.8790, -0.0025,  0.8709]],\n",
       "\n",
       "        [[-0.0233, -0.2503, -0.6902,  ..., -0.6061,  0.2298, -0.5997],\n",
       "         [ 1.0200, -1.0674, -2.2112,  ..., -0.7643, -0.2864,  0.3234],\n",
       "         [-1.2876,  0.6676, -0.1443,  ..., -1.0138,  0.1746,  0.9701]]])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp[mlp_c_proj][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 768])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp[mlp_c_proj][2].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "m2d2_env",
   "language": "python",
   "name": "m2d2_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
