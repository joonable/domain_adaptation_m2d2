{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 230623_sub_causal_tracing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다시 module 단위로 변경"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rds/general/user/jj1122/home/anaconda3/envs/m2d2_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast, GPT2Tokenizer, set_seed\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import torch\n",
    "import argparse\n",
    "import datasets\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "from datetime import date\n",
    "import sys\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(718)\n",
    "set_seed(718)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "omcd_econ_l1 omcd econ_l1\n"
     ]
    }
   ],
   "source": [
    "# job_cd = sys.argv[2]\n",
    "job_cd = \"omcd_econ_l1\"\n",
    "\n",
    "list_job_cd = job_cd.split(\"_\")\n",
    "job_gubun = list_job_cd[0]\n",
    "model_id = \"_\".join(list_job_cd[1:])\n",
    "print(job_cd, job_gubun, model_id)\n",
    "\n",
    "assert job_gubun in [\"omcd\", \"tmod\"]\n",
    "\n",
    "cache_dir = \"/rds/general/user/jj1122/ephemeral/.cache/huggingface\"\n",
    "device_id = \"cuda\"\n",
    "\n",
    "# n_layers = 12\n",
    "# list_modules = ['attn', 'mlp']\n",
    "# trace_module_id = \"transformer.h.{l}.{m}\"\n",
    "\n",
    "tuned_model_path = f\"/rds/general/user/jj1122/home/projects/m2d2/dataset/{model_id}/models\"\n",
    "\n",
    "today_dt = date.today().strftime(\"%y%m%d\")\n",
    "output_file = f\"/rds/general/user/jj1122/home/projects/m2d2/utils/output_logs/{today_dt}_{job_gubun}_{model_id}.json\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_dataset = pd.read_csv(\"./data/dataset_info_230709.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_dataset_nms =df_dataset.loc[(df_dataset.category == \"L1\") & (df_dataset.editing == True), \"dataset\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = \"/rds/general/user/jj1122/ephemeral/.cache/huggingface\"\n",
    "\n",
    "def parse_code_submodule(row):\n",
    "    list_code = row.code.split(\".\")\n",
    "    row[\"trace_id\"] = \".\".join(row[\"code\"].split(\".\")[:-1])\n",
    "#     row[\"component_id\"] = \".\".join(row[\"code\"].split(\".\")[1:-1])\n",
    "    is_in_layer = row[\"code\"].startswith(\"transformer.h\")\n",
    "\n",
    "    if is_in_layer:        \n",
    "        row[\"layer\"] = int(list_code[2])\n",
    "        row[\"module\"] = list_code[3]\n",
    "    else:\n",
    "        row[\"module\"] = list_code[1]\n",
    "        if row[\"module\"] == \"ln_f\": \n",
    "            row[\"layer\"] = int(99)\n",
    "        elif row[\"module\"] in [\"wte\", \"wpe\"]:\n",
    "            row['layer'] = int(-1)\n",
    "    \n",
    "    if row[\"module\"] in [\"attn\", \"mlp\"]:\n",
    "        row[\"submodule\"] = list_code[-2]\n",
    "    else:\n",
    "        row[\"submodule\"] = row[\"module\"]\n",
    "\n",
    "    row[\"w_or_b\"] = list_code[-1]\n",
    "    row[\"is_in_layer\"] = is_in_layer\n",
    "    \n",
    "    is_investigated = (not row[\"code\"].startswith(\"transformer.w\")) and (not row[\"submodule\"].startswith(\"ln\"))\n",
    "    row[\"is_investigated\"] = is_investigated\n",
    "\n",
    "    return row\n",
    "\n",
    "\n",
    "dict_n_parmas = {tup[0]: tup[1].numel() for tup in GPT2LMHeadModel.from_pretrained(\"gpt2\", cache_dir=cache_dir).to(\"cpu\").named_parameters()}\n",
    "df_submodule = pd.DataFrame.from_dict(dict_n_parmas, orient=\"index\").reset_index()\n",
    "df_submodule.columns = [\"code\", 'params']\n",
    "df_submodule = df_submodule.apply(lambda row: parse_code_submodule(row), axis=1)\n",
    "df_temp = df_submodule.groupby(\"trace_id\")[\"params\"].sum().to_frame().reset_index()\n",
    "df_temp.columns = [\"trace_id\", \"total\"]\n",
    "\n",
    "df_submodule[\"display_id\"] = df_submodule[\"layer\"].astype(str) + \".\" + df_submodule[\"module\"]\n",
    "\n",
    "df_submodule.loc[df_submodule.module != df_submodule.submodule, \"display_id\"] \\\n",
    "    = df_submodule.loc[df_submodule.module != df_submodule.submodule, \"display_id\"] \\\n",
    "        + \".\" + df_submodule.loc[df_submodule.module != df_submodule.submodule, \"submodule\"]\n",
    "\n",
    "df_submodule = df_submodule.merge(df_temp, on=[\"trace_id\"])\n",
    "# list_trace_ids = df_submodule.loc[df_submodule.is_investigated].trace_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>is_in_layer</th>\n",
       "      <th>is_investigated</th>\n",
       "      <th>layer</th>\n",
       "      <th>module</th>\n",
       "      <th>params</th>\n",
       "      <th>submodule</th>\n",
       "      <th>trace_id</th>\n",
       "      <th>w_or_b</th>\n",
       "      <th>display_id</th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>transformer.wte.weight</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>wte</td>\n",
       "      <td>38597376</td>\n",
       "      <td>wte</td>\n",
       "      <td>transformer.wte</td>\n",
       "      <td>weight</td>\n",
       "      <td>-1.wte</td>\n",
       "      <td>38597376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>transformer.wpe.weight</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>wpe</td>\n",
       "      <td>786432</td>\n",
       "      <td>wpe</td>\n",
       "      <td>transformer.wpe</td>\n",
       "      <td>weight</td>\n",
       "      <td>-1.wpe</td>\n",
       "      <td>786432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>transformer.h.0.ln_1.weight</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>ln_1</td>\n",
       "      <td>768</td>\n",
       "      <td>ln_1</td>\n",
       "      <td>transformer.h.0.ln_1</td>\n",
       "      <td>weight</td>\n",
       "      <td>0.ln_1</td>\n",
       "      <td>1536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>transformer.h.0.ln_1.bias</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>ln_1</td>\n",
       "      <td>768</td>\n",
       "      <td>ln_1</td>\n",
       "      <td>transformer.h.0.ln_1</td>\n",
       "      <td>bias</td>\n",
       "      <td>0.ln_1</td>\n",
       "      <td>1536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>transformer.h.0.attn.c_attn.weight</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>attn</td>\n",
       "      <td>1769472</td>\n",
       "      <td>c_attn</td>\n",
       "      <td>transformer.h.0.attn.c_attn</td>\n",
       "      <td>weight</td>\n",
       "      <td>0.attn.c_attn</td>\n",
       "      <td>1771776</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 code  is_in_layer  is_investigated  layer  \\\n",
       "0              transformer.wte.weight        False            False     -1   \n",
       "1              transformer.wpe.weight        False            False     -1   \n",
       "2         transformer.h.0.ln_1.weight         True            False      0   \n",
       "3           transformer.h.0.ln_1.bias         True            False      0   \n",
       "4  transformer.h.0.attn.c_attn.weight         True             True      0   \n",
       "\n",
       "  module    params submodule                     trace_id  w_or_b  \\\n",
       "0    wte  38597376       wte              transformer.wte  weight   \n",
       "1    wpe    786432       wpe              transformer.wpe  weight   \n",
       "2   ln_1       768      ln_1         transformer.h.0.ln_1  weight   \n",
       "3   ln_1       768      ln_1         transformer.h.0.ln_1    bias   \n",
       "4   attn   1769472    c_attn  transformer.h.0.attn.c_attn  weight   \n",
       "\n",
       "      display_id     total  \n",
       "0         -1.wte  38597376  \n",
       "1         -1.wpe    786432  \n",
       "2         0.ln_1      1536  \n",
       "3         0.ln_1      1536  \n",
       "4  0.attn.c_attn   1771776  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_submodule.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_module = df_submodule[:]\n",
    "\n",
    "df_module[\"trace_id\"] = df_module[\"trace_id\"]\\\n",
    "    .apply(lambda sub_trace_id: \".\".join(sub_trace_id.split(\".\")[:-1]) if sub_trace_id.split(\".\")[-2] in [\"attn\", \"mlp\"] else sub_trace_id)\n",
    "\n",
    "df_module = df_module\\\n",
    "    .groupby([\"trace_id\", \"is_in_layer\", \"is_investigated\", \"layer\", \"module\"], sort=False, as_index=False)\\\n",
    "    [[\"params\"]].sum()\n",
    "df_module[\"display_id\"] = df_module[\"layer\"].astype(str) + \".\" + df_module[\"module\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## list_trace_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_trace_ids = df_module.loc[df_module.is_investigated].trace_id.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset m2d2 (/rds/general/user/jj1122/ephemeral/.cache/huggingface/machelreid___m2d2/econ_l1/0.0.0/eb235f33a5de3163c10549b7f63c906910539c8a8c0ec5ade1285ccbf5067d00)\n",
      "100%|██████████| 3/3 [00:00<00:00, 37.94it/s]\n",
      "Loading cached processed dataset at /rds/general/user/jj1122/ephemeral/.cache/huggingface/machelreid___m2d2/econ_l1/0.0.0/eb235f33a5de3163c10549b7f63c906910539c8a8c0ec5ade1285ccbf5067d00/cache-1eec3cf03a698048.arrow\n"
     ]
    }
   ],
   "source": [
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\", cache_dir=cache_dir)\n",
    "dataset = load_dataset(\"machelreid/m2d2\", model_id, cache_dir=cache_dir)\n",
    "ds_test = dataset[\"test\"].filter(lambda x: x['text'] != '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /rds/general/user/jj1122/ephemeral/.cache/huggingface/machelreid___m2d2/econ_l1/0.0.0/eb235f33a5de3163c10549b7f63c906910539c8a8c0ec5ade1285ccbf5067d00/cache-12826ea30f123b74_*_of_00008.arrow\n"
     ]
    }
   ],
   "source": [
    "set_seed(718)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    output = gpt2_tokenizer(examples['text'], max_length=1024, truncation=True)\n",
    "    return output\n",
    "\n",
    "tokenized_datasets = ds_test.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    num_proc=8,\n",
    "    remove_columns='text', #TODO\n",
    "    load_from_cache_file=True,\n",
    ")\n",
    "\n",
    "len_sentences = len(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL & HOOK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL & HOOK\n",
    "def load_model(model=\"tuned\"):\n",
    "    if model == \"tuned\":\n",
    "        return GPT2LMHeadModel.from_pretrained(tuned_model_path).to(device_id)\n",
    "    else:\n",
    "        return GPT2LMHeadModel.from_pretrained(model, cache_dir=cache_dir).to(device_id)\n",
    "\n",
    "\n",
    "# def save_clean_activation(m_id):\n",
    "#     def save_clean_activation_hook(module, _input, _output):\n",
    "#         clean_activations[m_id] = _output.detach()\n",
    "#     return save_clean_activation_hook\n",
    "\n",
    "\n",
    "def corrupt_input_vector_v2(module, _input, _output):\n",
    "    torch.manual_seed(718)\n",
    "    std = torch.std(_output)\n",
    "    output = _output + (std*1.5) * torch.randn(_output[0].shape).to(device_id)\n",
    "    return output\n",
    "\n",
    "\n",
    "# def restore_activation(m_id):\n",
    "#     def restore_activation_hook(module, _input, _output):\n",
    "#         clean_activation = clean_activations[m_id]#[:, t]\n",
    "#         new_output = clean_activation\n",
    "#         del clean_activation\n",
    "#         print(f\"m_id: {m_id}\")\n",
    "#         print(f\"module: {module}\")\n",
    "#         print(f\"_input: {_input}\")\n",
    "#         print(f\"_output: {_output}\")\n",
    "#         print(f\"new_output: {new_output}\")\n",
    "#         return new_output \n",
    "#     return restore_activation_hook\n",
    "\n",
    "\n",
    "def save_clean_activation(m_id):\n",
    "    def save_clean_activation_hook(module, _input, _output):\n",
    "        if m_id.endswith('attn'):\n",
    "            clean_activations[m_id] = _output[0].detach()\n",
    "        elif m_id.endswith('mlp'):\n",
    "            clean_activations[m_id] = _output.detach()\n",
    "    return save_clean_activation_hook\n",
    "\n",
    "# def corrupt_input_vector(module, _input):  # , _output):\n",
    "#     std = torch.std(_input[0])\n",
    "#     return tuple([_input[0] + (std * 1.5) * torch.randn(_input[0].shape).to(device_id), ])\n",
    "\n",
    "def restore_activation(m_id):\n",
    "    def restore_activation_hook(module, _input, _output):\n",
    "        clean_activation = clean_activations[m_id]#[:, t]\n",
    "        if m_id.endswith('attn'):\n",
    "            return tuple([clean_activation, tuple([_output[1][0], _output[1][1]])])\n",
    "        elif m_id.endswith('mlp'):\n",
    "            # base_output = _output.detach()\n",
    "            # base_output = tuned_activation\n",
    "            return clean_activation\n",
    "    return restore_activation_hook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register Hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(718)\n",
    "\n",
    "if job_gubun == \"omcd\":\n",
    "    clean_model = load_model()\n",
    "    clean_model.eval()\n",
    "    for m_id in list_trace_ids:\n",
    "#         print(m_id)\n",
    "        clean_model.get_submodule(m_id).register_forward_hook(save_clean_activation(m_id))    \n",
    "\n",
    "    # Second run: corrupted run def\n",
    "    corrupted_model = load_model()\n",
    "    corrupted_model.eval()\n",
    "    corrupted_model.get_submodule(\"transformer.wte\").register_forward_hook(corrupt_input_vector_v2)\n",
    "\n",
    "    # Third run: restored run def    \n",
    "    restored_model = load_model()\n",
    "    restored_model.eval()\n",
    "    restored_model.get_submodule(\"transformer.wte\").register_forward_hook(corrupt_input_vector_v2)\n",
    "else:\n",
    "    clean_model = load_model()\n",
    "    clean_model.eval()\n",
    "    for m_id in list_trace_ids:\n",
    "        clean_model.get_submodule(m_id).register_forward_hook(save_clean_activation(m_id))\n",
    "\n",
    "    # Second run: base run def\n",
    "    corrupted_model = load_model(\"gpt2\")\n",
    "    corrupted_model.eval()\n",
    "\n",
    "    # Third run: restored run def\n",
    "    restored_model = load_model(\"gpt2\")\n",
    "    restored_model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Causal Tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with open(output_file, 'r') as json_file:\n",
    "        list_results = json.load(json_file)\n",
    "    for i, d in enumerate(list_results):\n",
    "        if len(d) == 0:\n",
    "            done_idx = i - 1\n",
    "            break\n",
    "except:\n",
    "    list_results = [{} for x in range(len_sentences)]\n",
    "    done_idx = 0\n",
    "print(done_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "done_idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(718)\n",
    "set_seed(718)\n",
    "\n",
    "for sentence_idx, data in enumerate(tokenized_datasets):    \n",
    "    if done_idx > sentence_idx: continue\n",
    "\n",
    "    if sentence_idx % 1000 == 0:\n",
    "        print(f\"sentence_idx: {sentence_idx}\")\n",
    "\n",
    "        \n",
    "    inputs = torch.tensor(data['input_ids']).to(device_id)\n",
    "\n",
    "    # First run: clean run\n",
    "    clean_activations = {}\n",
    "    with torch.no_grad():\n",
    "        clean_outputs = clean_model(inputs, labels=inputs.clone())\n",
    "        clean_loss = np.exp(clean_outputs.loss.item())\n",
    "\n",
    "    # Second run: corrupted run\n",
    "    with torch.no_grad():\n",
    "        corrupted_outputs = corrupted_model(inputs, labels=inputs.clone())\n",
    "        corrupted_loss = np.exp(corrupted_outputs.loss.item())\n",
    "\n",
    "#     Third run: corrupted-with-restoration run\n",
    "    restored_loss = {}\n",
    "    with torch.no_grad():\n",
    "        for m_id in list_trace_ids:\n",
    "            hook = restored_model.get_submodule(m_id).register_forward_hook(restore_activation(m_id))\n",
    "            restored_outputs = restored_model(inputs, labels=inputs.clone())        \n",
    "            restored_loss[m_id] = np.exp(restored_outputs.loss.item())\n",
    "            hook.remove()\n",
    "            \n",
    "    list_results[sentence_idx]['clean_loss'] = clean_loss\n",
    "    list_results[sentence_idx]['corrupted_loss'] = corrupted_loss\n",
    "    list_results[sentence_idx]['restored_loss'] = restored_loss\n",
    "\n",
    "    if sentence_idx % 1000 == 0:\n",
    "        with open(output_file, 'w') as json_file:\n",
    "            json.dump(list_results, json_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "m2d2_env",
   "language": "python",
   "name": "m2d2_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
