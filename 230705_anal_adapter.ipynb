{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rds/general/user/jj1122/home/anaconda3/envs/m2d2_adapter/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass, field\n",
    "from itertools import chain\n",
    "from typing import Optional\n",
    "import json\n",
    "from itertools import chain\n",
    "\n",
    "import datasets\n",
    "import evaluate\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    CONFIG_MAPPING,\n",
    "    MODEL_FOR_CAUSAL_LM_MAPPING,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    AdapterTrainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    "    is_torch_tpu_available,\n",
    "    set_seed,\n",
    "    AdapterArguments,\n",
    "    AdapterTrainer\n",
    ")\n",
    "from transformers.testing_utils import CaptureLogger\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from transformers.utils import check_min_version, send_example_telemetry\n",
    "from transformers.utils.versions import require_version\n",
    "from transformers import EarlyStoppingCallback\n",
    "from transformers.adapters import (\n",
    "    GPT2AdapterModel, AutoAdapterModel,  \n",
    "    AdapterConfig, PrefixTuningConfig, HoulsbyConfig, LoRAConfig\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# job_cd = sys.argv[2] # TODO\n",
    "# print(\"job_cd: \", job_cd)\n",
    "prj_path = \"/rds/general/user/jj1122/home/projects/domain_adaptation/\"\n",
    "cache_dir=\"/rds/general/user/jj1122/ephemeral/.cache/huggingface\"\n",
    "model_path_format = \"/rds/general/user/jj1122/home/projects/m2d2/dataset/{job_cd}/\"\n",
    "\n",
    "job_cd = \"bottleneck_adapter___Religion_and_belief_systems\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_id = 'cpu'\n",
    "n_layers = 12\n",
    "list_modules = ['ln_1', 'attn', 'ln_2', 'mlp']\n",
    "trace_module_id = \"transformer.h.{l}.{m}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 20)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_models = [\n",
    "    \"cs_l1\",\n",
    "    \"nlin_l1\",\n",
    "    \"Health_and_fitness\",\n",
    "    \"Natural_and_physical_sciences\",\n",
    "    \"Religion_and_belief_systems\",\n",
    "    \"Culture_and_the_arts\",\n",
    "    \"General_referece\",\n",
    "    \"econ_l1\",\n",
    "    \"History_and_events\",\n",
    "    \"Human_activites\",\n",
    "    \"Mathematics_and_logic\",\n",
    "    \"astro-ph_l1\",\n",
    "    \"cond-mat_l1\",\n",
    "#     \"eess_l1 (No Metric)\",\n",
    "    \"math_l1\",\n",
    "#     \"physics_l1 (ERROR)\",\n",
    "    \"q-bio_l1\",\n",
    "#     \"q-fin_l1 (No Metric)\",\n",
    "    \"stat_l1\",\n",
    "    \"Philosophy\",\n",
    "    \"Philosophy_and_thinking\",\n",
    "    \"Society_and_social_sciences\",\n",
    "    \"Technology_and_applied_sciences\",\n",
    "#     \"art_l1 (No Data)\"\n",
    "]\n",
    "len(list_models), len(set(list_models))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_model_path(model_id):\n",
    "#     model_path_format = \"/rds/general/user/jj1122/home/projects/m2d2/dataset/{model_id}/models\"\n",
    "#     ckpt_path_format = \"/checkpoint-{ckpt}\"\n",
    "\n",
    "#     if model_id == \"gpt2\":\n",
    "#         model_path = \"gpt2\"\n",
    "# #         ckpt = \"zs\"\n",
    "#     else:\n",
    "#         model_path = model_path_format.format(model_id=model_id)\n",
    "#         l_dir = listdir(model_path)\n",
    "\n",
    "#         if all([len(x.split(\".\")) == 1 for x in l_dir]):\n",
    "#             ckpt = max([int(x.split(\"-\")[1]) for x in l_dir])\n",
    "#             model_path += ckpt_path_format.format(ckpt=ckpt)\n",
    "# #         else:\n",
    "# #             ckpt = \"final\"\n",
    "#     return model_path\n",
    "\n",
    "# config_models = {model_id: {} for model_id in list_models}\n",
    "# for model_id in list_models:\n",
    "#     config_models[model_id][\"model_path\"] = get_model_path(model_id)\n",
    "#     json_path = config_models[model_id][\"model_path\"] + \"/trainer_state.json\"\n",
    "#     with open(json_path, \"r\") as json_file:\n",
    "#         trainer_state = json.load(json_file)\n",
    "#         config_models[model_id][\"val_ppl\"] = math.exp(trainer_state[\"best_metric\"])\n",
    "#         config_models[model_id][\"train_steps\"] = int(trainer_state[\"best_model_checkpoint\"].split(\"/\")[-1].split(\"-\")[-1])\n",
    "#         config_models[model_id][\"train_eps\"] = float(trainer_state[\"epoch\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/rds/general/user/jj1122/home/projects/m2d2/dataset/bottleneck_adapter___Religion_and_belief_systems/'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path_format.format(job_cd=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2AdapterModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.11.attn.masked_bias', 'h.2.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.4.attn.masked_bias', 'h.10.attn.masked_bias', 'h.6.attn.masked_bias', 'h.0.attn.masked_bias', 'h.3.attn.masked_bias', 'h.7.attn.masked_bias', 'h.5.attn.masked_bias', 'h.1.attn.masked_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of GPT2AdapterModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.11.attn.masked_bias', 'h.2.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.4.attn.masked_bias', 'h.10.attn.masked_bias', 'h.6.attn.masked_bias', 'h.0.attn.masked_bias', 'h.3.attn.masked_bias', 'h.7.attn.masked_bias', 'h.5.attn.masked_bias', 'h.1.attn.masked_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "base_model =  GPT2LMHeadModel.from_pretrained(\"gpt2\", cache_dir=cache_dir)\n",
    "tuned_model_1 = GPT2AdapterModel.from_pretrained(\"gpt2\", cache_dir=cache_dir)\n",
    "tuned_model_1.add_adapter(model_path_format.format(job_cd=job_cd))\n",
    "tuned_model_2 = GPT2AdapterModel.from_pretrained(\"gpt2\", cache_dir=cache_dir)\n",
    "tuned_model_2.add_adapter(model_path_format.format(job_cd=\"bottleneck_adapter___History_and_events\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2AdapterModel(\n",
       "  (shared_parameters): ModuleDict()\n",
       "  (transformer): GPT2Model(\n",
       "    (shared_parameters): ModuleDict()\n",
       "    (invertible_adapters): ModuleDict()\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): MergedLinear(\n",
       "            in_features=768, out_features=2304, bias=True\n",
       "            (loras): ModuleDict()\n",
       "          )\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (prefix_tuning): PrefixTuningShim(\n",
       "            (prefix_gates): ModuleDict()\n",
       "            (pool): PrefixTuningPool(\n",
       "              (prefix_tunings): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Linear(\n",
       "            in_features=768, out_features=3072, bias=True\n",
       "            (loras): ModuleDict()\n",
       "          )\n",
       "          (c_proj): Linear(\n",
       "            in_features=3072, out_features=768, bias=True\n",
       "            (loras): ModuleDict()\n",
       "          )\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (attention_adapters): AdapterLayer(\n",
       "          (adapters): ModuleDict()\n",
       "          (adapter_fusion_layer): ModuleDict()\n",
       "        )\n",
       "        (output_adapters): AdapterLayer(\n",
       "          (adapters): ModuleDict(\n",
       "            (/rds/general/user/jj1122/home/projects/m2d2/dataset/bottleneck_adapter___Religion_and_belief_systems/): Adapter(\n",
       "              (non_linearity): Activation_Function_Class(\n",
       "                (f): ReLU()\n",
       "              )\n",
       "              (adapter_down): Sequential(\n",
       "                (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                (1): Activation_Function_Class(\n",
       "                  (f): ReLU()\n",
       "                )\n",
       "              )\n",
       "              (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (adapter_fusion_layer): ModuleDict()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (prefix_tuning): PrefixTuningPool(\n",
       "      (prefix_tunings): ModuleDict()\n",
       "    )\n",
       "  )\n",
       "  (heads): ModuleDict()\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuned_model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.h.0.output_adapters.adapters./rds/general/user/jj1122/home/projects/m2d2/dataset/bottleneck_adapter___Religion_and_belief_systems/.adapter_down.0.bias tensor(True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True) Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for tup1, tup2 in zip(tuned_model_1.named_parameters(), tuned_model_2.named_parameters()):\n",
    "    if \"adapters\" in tup1[0] and \"bias\" in tup1[0]:\n",
    "        print(tup1[0], torch.all(tup1[1] == tup2[1]))\n",
    "        print(tup1[1], tup2[1])\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_model = AutoAdapterModel(model_path_format.format(job_cd=job_cd), cache_dir=cache_dir).to(device_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "m2d2_adapter",
   "language": "python",
   "name": "m2d2_adapter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
